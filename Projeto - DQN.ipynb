{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EPISODES = 30000 #For pacman\n",
    "#EPISODES = 1000 #For CarPole\n",
    "\n",
    "memory = deque(maxlen=2000)\n",
    "gamma = 0.99    \n",
    "epsilon = 1.0 \n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = (1-0.01)/30000 #For Pacman\n",
    "#epsilon_decay = (1-0.01)/1000 #For CartPole\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def downsample(img):\n",
    "    return img[::2, ::2]\n",
    "\n",
    "def resize(img):\n",
    "    return img[0:170,:]\n",
    "\n",
    "def preprocess(img):\n",
    "    return to_grayscale(resize(img))\n",
    "\n",
    "def change(img):\n",
    "    img = preprocess(img)\n",
    "    img = np.reshape(img, (170, 160, 1))\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    return img\n",
    "\n",
    "def build_model():\n",
    "        \n",
    "    # For Cartpole\n",
    "    #model = Sequential()    \n",
    "    #model.add(Dense(20, input_dim = state_size , init='uniform', activation='relu'))\n",
    "    #model.add(Dense(40, activation='relu'))     \n",
    "    #model.add(Dense(40, activation='relu'))\n",
    "    #model.add(Dense(20, init='uniform', activation='relu'))\n",
    "    #model.add(Dense(env.action_space.n, init='uniform', activation='linear'))    \n",
    "    #model.compile(loss='mean_squared_error',optimizer=Adam(learning_rate))\n",
    "        \n",
    "    #return model\n",
    "       \n",
    "        \n",
    "    #For Pacman\n",
    "    model = Sequential()    \n",
    "    model.add(Dense(128, input_dim = self.state_size , init='uniform', activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(128, activation='relu'))  \n",
    "    model.add(Dropout(0.5))    \n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(128, init='uniform', activation='relu'))\n",
    "    model.add(Dense(env.action_space.n, init='uniform', activation='linear'))    \n",
    "    model.compile(loss='mean_squared_error',optimizer=Adam(lr=learning_rate))\n",
    "        \n",
    "    return model\n",
    "\n",
    "def get_model_weights():\n",
    "    # copy weights from model to target_model\n",
    "    target_model.set_weights(model.get_weights())\n",
    "\n",
    "def remember(state, action, reward, next_state, done):\n",
    "    memory.append((state, action, reward, next_state, done)) #Store in memory to do experience replay\n",
    "\n",
    "def get_action(state):\n",
    "    if np.random.rand() <= epsilon: #Chance of getting random move\n",
    "        return random.randrange(action_size)\n",
    "    actions = model.predict(state)\n",
    "    return np.argmax(actions[0]) \n",
    "\n",
    "def experience_replay(batch_size):\n",
    "    global epsilon\n",
    "    minibatch = random.sample(memory, batch_size)\n",
    "    i = 0\n",
    "    for state, action, reward, next_state, done in minibatch:\n",
    "        i = i +1\n",
    "        target = model.predict(state)\n",
    "        if done:\n",
    "            target[0][action] = reward\n",
    "        else:\n",
    "            t = target_model.predict(next_state)[0]\n",
    "            target[0][action] = reward + gamma * np.amax(t)\n",
    "\n",
    "        model.fit(state, target, epochs=1, verbose = 0)\n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon = epsilon - epsilon_decay\n",
    "\n",
    "def load(name):\n",
    "    model.load_weights(name)\n",
    "\n",
    "def save(self, name):\n",
    "    model.save_weights(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Playing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env = gym.make('CartPole-v0') # Get the game environment\n",
    "env = gym.make('MsPacman-ram-v0') # Get the game environment\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "done = False\n",
    "batch_size = 32\n",
    "model = build_model()\n",
    "target_model = build_model()\n",
    "scores = []\n",
    "\n",
    "for e in range(EPISODES):\n",
    "    state = env.reset() #Start a new game\n",
    "    state = state/255 #If cartPole comment this line\n",
    "    state = np.reshape(state,[1,state_size])\n",
    "    tot_reward = 0.0\n",
    "    done = False\n",
    "    while not done:\n",
    "        # env.render()\n",
    "        action = get_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        if reward == 0:               #\n",
    "            reward = -0.5             # If Cartpole, comment all these lines\n",
    "        next_state = next_state/255   #\n",
    "        \n",
    "        next_state = np.reshape(next_state,[1,state_size])\n",
    "        remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        tot_reward = tot_reward + reward\n",
    "        if done:\n",
    "            get_model_weights()\n",
    "            scores.append(tot_reward)\n",
    "            print(\"episode: {}/{}, score: {}, e: {:.2}\".format(e, EPISODES, tot_reward, epsilon))\n",
    "            break\n",
    "            \n",
    "    if(len(memory) > batch_size):\n",
    "        experience_replay(batch_size)\n",
    "        agent.save(\"random.h5\") #Save the net weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the graph\n",
    "\n",
    "pay attention to not overwrite graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[8,6])\n",
    "plt.plot(range(len(scores)),scores,linewidth=3.0)\n",
    "plt.xlabel('Episode ',fontsize=16)\n",
    "plt.ylabel('Total reward',fontsize=16)\n",
    "plt.title('Reward per Episode - Pacman',fontsize=16)\n",
    "plt.savefig('PacmanGraph')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fram = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(fram)\n",
    "plt.savefig('Pacmangame')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
